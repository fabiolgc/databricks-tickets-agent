# Guia: Feature Store e AutoML para Predi√ß√£o de Churn

## üìã Vis√£o Geral

Este guia demonstra como criar uma Feature Store no Databricks e usar o AutoML para treinar um modelo de predi√ß√£o de churn baseado em dados de tickets de suporte.

## üéØ Objetivo

Criar um modelo de machine learning que identifique empresas com alto risco de churn baseado em:
- Volume e status de tickets
- Satisfa√ß√£o do cliente (NPS, CSAT)
- Tempo de resolu√ß√£o
- Sentimento das intera√ß√µes
- Tend√™ncias temporais

## üìÅ Arquivos Criados

### 1. `churn_feature_store.py`
Notebook principal que cria a Feature Store com 60+ features.

**Features geradas:**
- **Volume**: total de tickets, tickets abertos/fechados, por prioridade
- **Satisfa√ß√£o**: NPS m√©dio, CSAT m√©dio, distribui√ß√£o de sentimentos
- **Tempo**: tempo de resolu√ß√£o, tempo de primeira resposta
- **Tend√™ncias**: tickets nos √∫ltimos 30/60/90 dias
- **Qualidade**: taxa de breach de SLA, taxa de reclama√ß√µes
- **Derivadas**: taxas de resolu√ß√£o, tickets por cliente

### 2. `automl_churn_training.py`
Notebook para treinar modelo usando Databricks AutoML.

**Funcionalidades:**
- Executa AutoML com m√∫ltiplos algoritmos
- Analisa feature importance
- Registra melhor modelo no MLflow
- Gera predi√ß√µes em batch

### 3. `feature_store_refresh_job.py`
Job automatizado para atualizar a Feature Store periodicamente.

**Modos de execu√ß√£o:**
- `full`: recalcula todas as features
- `incremental`: atualiza apenas dados recentes

## üöÄ Como Usar

### Passo 1: Preparar Ambiente

1. **Carregar dados no Databricks**:
```sql
-- Criar schema
CREATE SCHEMA IF NOT EXISTS main.ticket_analytics;

-- Carregar tabelas (ajuste os caminhos)
CREATE TABLE main.ticket_analytics.companies AS 
SELECT * FROM csv.`/path/to/companies.csv`;

CREATE TABLE main.ticket_analytics.customers AS 
SELECT * FROM csv.`/path/to/customers.csv`;

CREATE TABLE main.ticket_analytics.tickets AS 
SELECT * FROM csv.`/path/to/tickets.csv`;

CREATE TABLE main.ticket_analytics.ticket_interactions AS 
SELECT * FROM csv.`/path/to/ticket_interactions.csv`;
```

### Passo 2: Criar Feature Store

1. Abra o notebook `churn_feature_store.py` no Databricks
2. Ajuste as vari√°veis de configura√ß√£o:
   ```python
   CATALOG = "main"
   SCHEMA = "ticket_analytics"
   ```
3. Execute todos os comandos do notebook
4. Verifique se as tabelas foram criadas:
   - `main.ticket_analytics.company_churn_features`
   - `main.ticket_analytics.company_churn_training_data`
   - `main.ticket_analytics.churn_feature_metadata`

### Passo 3: Treinar Modelo com AutoML

#### Op√ß√£o A: Usando o Notebook

1. Abra `automl_churn_training.py`
2. Execute o notebook completo
3. O AutoML vai:
   - Testar m√∫ltiplos algoritmos (Random Forest, XGBoost, LightGBM, etc.)
   - Otimizar hiperpar√¢metros
   - Selecionar o melhor modelo
   - Registrar no MLflow

#### Op√ß√£o B: Usando a UI do Databricks

1. V√° em **Machine Learning** > **AutoML**
2. Clique em **Start AutoML**
3. Configure:
   - **Tabela**: `main.ticket_analytics.company_churn_training_data`
   - **Problema**: Classification
   - **Target**: `is_churn_risk`
   - **M√©trica**: F1 Score
   - **Timeout**: 30 minutos
4. Clique em **Start**

### Passo 4: Avaliar Resultados

Ap√≥s o treinamento, verifique:

1. **Experimentos no MLflow**:
   - V√° em **Machine Learning** > **Experiments**
   - Encontre seu experimento de churn
   - Compare m√©tricas dos modelos

2. **Feature Importance**:
   - No notebook AutoML, veja o gr√°fico de import√¢ncia
   - Identifique quais features mais impactam o churn

3. **M√©tricas do Modelo**:
   ```
   - Accuracy: % de predi√ß√µes corretas
   - Precision: % de predi√ß√µes positivas corretas
   - Recall: % de casos positivos identificados
   - F1 Score: m√©dia harm√¥nica de precision e recall
   - ROC AUC: capacidade de separar classes
   ```

### Passo 5: Usar o Modelo em Produ√ß√£o

#### Fazer Predi√ß√µes em Batch

```python
import mlflow

# Carregar modelo
model = mlflow.sklearn.load_model("models:/company_churn_prediction/latest")

# Carregar dados
companies_df = spark.table("main.ticket_analytics.company_churn_training_data")

# Fazer predi√ß√µes
predictions = model.predict(companies_df.drop("is_churn_risk").toPandas())

# Salvar resultados
result_df = companies_df.select("company_id", "company_name")
result_df = result_df.withColumn("predicted_churn_risk", predictions)
result_df.write.format("delta").mode("overwrite").saveAsTable(
    "main.ticket_analytics.company_churn_predictions"
)
```

#### Criar Endpoint de Real-Time

1. V√° em **Machine Learning** > **Models**
2. Selecione `company_churn_prediction`
3. Clique em **Serve Model**
4. Configure o endpoint:
   - **Compute**: Small (1-2 cores)
   - **Scale**: 1-5 inst√¢ncias
5. Use a API:

```python
import requests
import json

url = "https://<databricks-instance>/serving-endpoints/company_churn_prediction/invocations"
headers = {"Authorization": f"Bearer {token}"}

data = {
    "dataframe_records": [{
        "total_tickets": 45,
        "tickets_open": 5,
        "avg_nps_score": 6.5,
        "avg_csat_score": 3.8,
        "sla_breach_rate": 0.15,
        # ... outras features
    }]
}

response = requests.post(url, headers=headers, json=data)
prediction = response.json()
```

### Passo 6: Agendar Atualiza√ß√£o Autom√°tica

1. V√° em **Workflows** > **Jobs**
2. Clique em **Create Job**
3. Configure:
   - **Task**: `feature_store_refresh_job`
   - **Notebook**: `notebooks/feature_store_refresh_job.py`
   - **Cluster**: Shared cluster ou job cluster
   - **Schedule**: Daily √†s 2:00 AM
   - **Parameters**:
     ```json
     {
       "refresh_mode": "incremental",
       "lookback_days": "7"
     }
     ```
4. Salve e ative o job

## üìä M√©tricas de Features

### Features Mais Importantes (t√≠picas)

1. **tickets_last_30_days**: Atividade recente
2. **sla_breach_rate**: Qualidade do servi√ßo
3. **avg_nps_score**: Satisfa√ß√£o geral
4. **tickets_complaint**: Insatisfa√ß√£o expl√≠cita
5. **negative_sentiment_rate**: Sentimento negativo
6. **ticket_resolution_rate**: Efici√™ncia
7. **days_since_last_ticket**: Engajamento
8. **tickets_churn_risk_tag**: Indicador direto

### Interpreta√ß√£o de Predi√ß√µes

| Score | Risco | A√ß√£o Recomendada |
|-------|-------|------------------|
| 0.0 - 0.3 | Baixo | Manter relacionamento padr√£o |
| 0.3 - 0.5 | M√©dio | Monitorar de perto |
| 0.5 - 0.7 | Alto | Contato proativo do CSM |
| 0.7 - 1.0 | Cr√≠tico | Interven√ß√£o urgente da lideran√ßa |

## üîç Monitoramento e Manuten√ß√£o

### 1. Monitorar Feature Drift

```python
# Comparar distribui√ß√£o de features ao longo do tempo
from databricks.feature_store import FeatureStoreClient

fs = FeatureStoreClient()

# Feature atual vs hist√≥rica
current = spark.table("main.ticket_analytics.company_churn_features")
historical = spark.table("main.ticket_analytics.company_churn_features@v100")

# Comparar estat√≠sticas
current.select("avg_nps_score").summary().show()
historical.select("avg_nps_score").summary().show()
```

### 2. Monitorar Performance do Modelo

```python
# Calcular m√©tricas em dados recentes
from sklearn.metrics import classification_report

recent_data = spark.table("main.ticket_analytics.company_churn_predictions") \
    .filter("prediction_timestamp > current_date() - interval 7 days")

# Comparar predi√ß√µes vs realidade
# (necessita de label de churn real ap√≥s algumas semanas)
```

### 3. Retreinar o Modelo

Retreine quando:
- Accuracy cair > 5%
- Feature drift significativo
- Novos padr√µes de neg√≥cio
- A cada 3-6 meses (m√≠nimo)

## üõ†Ô∏è Troubleshooting

### Erro: "Feature table already exists"

```python
# Deletar e recriar
spark.sql("DROP TABLE IF EXISTS main.ticket_analytics.company_churn_features")
# Execute o notebook novamente
```

### Erro: "Memory error during AutoML"

- Reduza o dataset: `training_df.sample(0.5)`
- Aumente o cluster size
- Reduza `max_trials` no AutoML

### Features com muitos valores nulos

```python
# Investigar features espec√≠ficas
feature_df.select("avg_nps_score").filter(col("avg_nps_score").isNull()).count()

# Adicionar mais imputa√ß√£o no notebook
feature_df = feature_df.fillna({"avg_nps_score": 0})
```

## üìö Pr√≥ximos Passos

1. **Enriquecer Features**:
   - Adicionar dados de faturamento
   - Incluir uso do produto
   - Hist√≥rico de pagamentos

2. **Segmenta√ß√£o**:
   - Criar modelos espec√≠ficos por segmento
   - Diferentes thresholds por company_size

3. **Explicabilidade**:
   - Usar SHAP values
   - Criar dashboards de interpreta√ß√£o

4. **Integra√ß√£o**:
   - Alertas autom√°ticos no Slack/Email
   - Dashboard de risco de churn
   - CRM integration

## üìû Suporte

Para d√∫vidas:
- Documenta√ß√£o Databricks: https://docs.databricks.com/
- Feature Store: https://docs.databricks.com/machine-learning/feature-store/
- AutoML: https://docs.databricks.com/machine-learning/automl/

---

**Criado por**: Fabio Gon√ßalves  
**Data**: Janeiro 2026  
**Vers√£o**: 1.0
